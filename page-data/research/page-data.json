{"componentChunkName":"component---gatsby-theme-academic-src-pages-research-index-jsx","path":"/research/","result":{"data":{"allTag":{"edges":[{"node":{"name":"deep-learning","color":"magenta","path":"/tags/deep-learning"}},{"node":{"name":"computer-vision","color":"magenta","path":"/tags/computer-vision"}},{"node":{"name":"neural-networks","color":"red","path":"/tags/neural-networks"}},{"node":{"name":"research","color":"red","path":"/tags/research"}},{"node":{"name":"machine-learning","color":"green","path":"/tags/machine-learning"}},{"node":{"name":"language-models","color":"lime","path":"/tags/language-models"}},{"node":{"name":"optimization","color":"cyan","path":"/tags/optimization"}},{"node":{"name":"neurips","color":"cyan","path":"/tags/neurips"}},{"node":{"name":"super-resolution","color":"geekblue","path":"/tags/super-resolution"}},{"node":{"name":"ddpm","color":"orange","path":"/tags/ddpm"}},{"node":{"name":"regularization","color":"blue","path":"/tags/regularization"}},{"node":{"name":"resnet","color":"blue","path":"/tags/resnet"}},{"node":{"name":"autonomous-robotics","color":"volcano","path":"/tags/autonomous-robotics"}},{"node":{"name":"flight","color":"gold","path":"/tags/flight"}},{"node":{"name":"ardupilot","color":"volcano","path":"/tags/ardupilot"}},{"node":{"name":"embedded-systems","color":"gold","path":"/tags/embedded-systems"}},{"node":{"name":"metaheuristic-optimization","color":"green","path":"/tags/metaheuristic-optimization"}}]},"allMdx":{"edges":[{"node":{"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/site/static/d47952e4bd91c70708919d79f77a2346/3ea76/lora_preview.png","srcSet":"/site/static/d47952e4bd91c70708919d79f77a2346/3ea76/lora_preview.png 320w,\n/site/static/d47952e4bd91c70708919d79f77a2346/a2ff0/lora_preview.png 712w","sizes":"(min-width: 320px) 320px, 100vw"},"sources":[{"srcSet":"/site/static/d47952e4bd91c70708919d79f77a2346/4a8d5/lora_preview.webp 320w,\n/site/static/d47952e4bd91c70708919d79f77a2346/76729/lora_preview.webp 712w","type":"image/webp","sizes":"(min-width: 320px) 320px, 100vw"}]},"width":320,"height":180}}}},"fields":{"slug":{"date":"1733011200000","venue":"ICLR Workshop 2025","authors":["[Liam Cawley](https://cawleyl.github.io)"],"path":"research/lorambo","title":"LoRAMBo: Fighting LoRA Memory Bottlenecks with Optimized Rank Selection","tags":["deep-learning","machine-learning","language-models","optimization","neurips"],"excerpt":"Low-Rank Adaptation (LoRA) efficiently fine-tunes large pre-trained language models through low-rank weight updates, significantly reducing memory usage compared to full fine-tuning. However, the problem of how to optimally allocate low-rank parameters across model layers remains challenging. This paper presents an extended theoretical framework that unifies classical matrix approximation perspectives with data- and curvature-aware approaches, developing both offline and online rank-allocation algorithms with near-optimality guarantees. Our results significantly advance the theoretical understanding of efficient model adaptation while providing strong empirical evidence for adopting curvature- and data-aware rank selection strategies in large-scale applications.","priority":0,"links":[{"name":"paper","url":"/files/6656fcf836373e7122f4a7f83cdf7beb/lora_master.pdf"},{"name":"code","url":"https://github.com/cawley/lorambo"}]}},"internal":{"contentFilePath":"/home/runner/work/site/site/example/content/research/lorambo/index.md"}}},{"node":{"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/site/static/83860d6df73cb81ab02415b9869376af/3ea76/emag_preview.png","srcSet":"/site/static/83860d6df73cb81ab02415b9869376af/3ea76/emag_preview.png 320w,\n/site/static/83860d6df73cb81ab02415b9869376af/2c755/emag_preview.png 750w,\n/site/static/83860d6df73cb81ab02415b9869376af/aeac1/emag_preview.png 838w","sizes":"(min-width: 320px) 320px, 100vw"},"sources":[{"srcSet":"/site/static/83860d6df73cb81ab02415b9869376af/4a8d5/emag_preview.webp 320w,\n/site/static/83860d6df73cb81ab02415b9869376af/a36ff/emag_preview.webp 750w,\n/site/static/83860d6df73cb81ab02415b9869376af/4b29b/emag_preview.webp 838w","type":"image/webp","sizes":"(min-width: 320px) 320px, 100vw"}]},"width":320,"height":180}}}},"fields":{"slug":{"date":"2023-07","venue":"EMag Technologies, Inc.","authors":["Liam Cawley","Gabe Ronan"],"path":"research/emag_tech","title":"Novel Optimizer for Phased Array Calibration","tags":["metaheuristic-optimization","computer-vision","neural-networks","research"],"excerpt":"This Python repository focuses on creating a calibrator application that optimizes the amplitude of a [phased array](https://en.wikipedia.org/wiki/Phased_array#:~:text=In%20antenna%20theory%2C%20a%20phased,directions%20without%20moving%20the%20antennas) by adjusting the phase shift and attenuator positions of [Anokiwave AWS-0103](https://www.anokiwave.com/products/aws-0103/index.html) beamformers. The calibrator accepts binary files representing the system state of each AWS-0103 alters phase shifter position, gain and attenuation values to calibrate each beamformer, and consequently the phased array beam, in a user-defined configuration. These two papers provide a [simple overview](https://web2.norsonic.com/wp-content/uploads/2016/10/TN-beamformers.pdf) and a more [in-depth look](https://sci-hub.ru/10.1109/8.923310) at beamformers and how they are calibrated.","priority":0,"links":[]}},"internal":{"contentFilePath":"/home/runner/work/site/site/example/content/research/emag_tech/index.md"}}},{"node":{"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8d8d8","images":{"fallback":{"src":"/site/static/c0af3835955219f9d35b390e3df17a2a/3ea76/ddpm_preview.png","srcSet":"/site/static/c0af3835955219f9d35b390e3df17a2a/3ea76/ddpm_preview.png 320w,\n/site/static/c0af3835955219f9d35b390e3df17a2a/2c755/ddpm_preview.png 750w,\n/site/static/c0af3835955219f9d35b390e3df17a2a/1ec6c/ddpm_preview.png 1080w,\n/site/static/c0af3835955219f9d35b390e3df17a2a/966f7/ddpm_preview.png 1298w","sizes":"(min-width: 320px) 320px, 100vw"},"sources":[{"srcSet":"/site/static/c0af3835955219f9d35b390e3df17a2a/4a8d5/ddpm_preview.webp 320w,\n/site/static/c0af3835955219f9d35b390e3df17a2a/a36ff/ddpm_preview.webp 750w,\n/site/static/c0af3835955219f9d35b390e3df17a2a/afc13/ddpm_preview.webp 1080w,\n/site/static/c0af3835955219f9d35b390e3df17a2a/efa01/ddpm_preview.webp 1298w","type":"image/webp","sizes":"(min-width: 320px) 320px, 100vw"}]},"width":320,"height":180}}}},"fields":{"slug":{"date":"1714435200000","venue":"University Research Project","authors":["Liam Cawley","Alexandra Lavacek","Sophia Tesic"],"path":"research/super-resolution-ddpm","title":"DDPM for Super Resolution","tags":["deep-learning","computer-vision","super-resolution","ddpm","neural-networks","research"],"excerpt":"This research explores novel approaches for single image super resolution using deep learning techniques inspired by Denoising Diffusion Probabilistic Models (DDPM). We introduce several unique features and modifications that adapt DDPM concepts to the super resolution domain. Our goal was to exceed baseline bicubic interpolation performance through architectural innovations combining residual blocks, channel attention mechanisms, and perceptual loss functions.","priority":0,"links":[{"name":"pdf","url":"/files/db7e6d8a2134ed8de8bf466b3d37e9b3/442_final_report-combined.pdf"}]}},"internal":{"contentFilePath":"/home/runner/work/site/site/example/content/research/super-resolution-ddpm/index.md"}}},{"node":{"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a87838","images":{"fallback":{"src":"/site/static/20add60776689b008cb446676c7e39b2/3ea76/UAV_preview.png","srcSet":"/site/static/20add60776689b008cb446676c7e39b2/3ea76/UAV_preview.png 320w,\n/site/static/20add60776689b008cb446676c7e39b2/177c6/UAV_preview.png 545w","sizes":"(min-width: 320px) 320px, 100vw"},"sources":[{"srcSet":"/site/static/20add60776689b008cb446676c7e39b2/4a8d5/UAV_preview.webp 320w,\n/site/static/20add60776689b008cb446676c7e39b2/286df/UAV_preview.webp 545w","type":"image/webp","sizes":"(min-width: 320px) 320px, 100vw"}]},"width":320,"height":180}}}},"fields":{"slug":{"date":"1617494400000","venue":"WRAM Field, Patterson, NY","authors":["Liam Cawley"],"path":"research/UAV_build","title":"AutoUAV Build","tags":["autonomous-robotics","flight","ardupilot","embedded-systems"],"excerpt":"An autonomous UAV system built on the MFD Crosswind Mini platform, designed to achieve Level 5 autonomy for agricultural and mapping applications. This project demonstrates advanced autonomous flight capabilities through integration of computer vision, GPS navigation, and real-time telemetry.","priority":0,"links":[{"name":"pdf","url":"/files/669fa7ff45fa4a845a8e03674fa42b9c/UAVPaper_2021.pdf"},{"name":"website","url":"https://www.cawleyl.medium.com"}]}},"internal":{"contentFilePath":"/home/runner/work/site/site/example/content/research/UAV_build/index.md"}}},{"node":{"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/3ea76/shake_preview.png","srcSet":"/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/3ea76/shake_preview.png 320w,\n/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/2c755/shake_preview.png 750w,\n/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/556d7/shake_preview.png 759w","sizes":"(min-width: 320px) 320px, 100vw"},"sources":[{"srcSet":"/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/4a8d5/shake_preview.webp 320w,\n/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/a36ff/shake_preview.webp 750w,\n/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/fcee6/shake_preview.webp 759w","type":"image/webp","sizes":"(min-width: 320px) 320px, 100vw"}]},"width":320,"height":180}}}},"fields":{"slug":{"date":"1701388800000","venue":"University Research Project under Qing Qu","authors":["[Liam Cawley](mailto:cawleyl@umich.edu)"],"path":"research/shake-shake-regularization","title":"Comparative Analysis of Shake-Shake Regularization in a ResNet-Like Architecture for CIFAR-10 Image Classification","tags":["deep-learning","regularization","resnet","neural-networks","computer-vision"],"excerpt":"Investigation of effectiveness of non-ERM based methods in improving ResNet performance on image classification tasks. In the field of machine learning, especially in image classification, the challenge of overfitting and underperformance on new data is persistent. This research addresses the critical need for effective regularization techniques that enhance the generalization ability of models without compromising their performance. Utilizing the CIFAR-10 dataset, we evaluate the effectiveness of techniques such as Shake-Shake, Mixup, and Cutout in improving the performance of Convolutional Neural Networks (CNNs) and Residual Networks (ResNet).","priority":0,"links":[{"name":"paper","url":"/files/3bf41cf28ee3327689c4a570724dd670/shake-shake.pdf"},{"name":"code","url":"https://github.com/"}]}},"internal":{"contentFilePath":"/home/runner/work/site/site/example/content/research/shake-shake-regularization/index.md"}}}]}},"pageContext":{"slug":"/research/","langKey":"en"}},"staticQueryHashes":["1552981879","2158328490","3013679938"],"slicesMap":{}}