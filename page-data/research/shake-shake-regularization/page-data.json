{"componentChunkName":"component---gatsby-theme-academic-src-templates-post-post-jsx-content-file-path-content-research-shake-shake-regularization-index-md","path":"/research/shake-shake-regularization/","result":{"data":{"mdx":{"tableOfContents":{"items":[{"url":"#advanced-regularization-techniques-for-neural-networks-","title":"Advanced Regularization Techniques for Neural Networks üß†","items":[{"url":"#introduction-","title":"Introduction üéØ"},{"url":"#project-overview-","title":"Project Overview üìä","items":[{"url":"#key-features-","title":"Key Features üåü"}]},{"url":"#technical-implementation-","title":"Technical Implementation üíª","items":[{"url":"#basic-cnn-architecture","title":"Basic CNN Architecture"},{"url":"#advanced-regularization-implementations","title":"Advanced Regularization Implementations","items":[{"url":"#cutout","title":"Cutout"},{"url":"#mixup","title":"Mixup"},{"url":"#shake-shake-regularization","title":"Shake-Shake Regularization"}]}]},{"url":"#performance-results-","title":"Performance Results üìà","items":[{"url":"#key-findings-","title":"Key Findings üîç"}]},{"url":"#getting-started-","title":"Getting Started üöÄ","items":[{"url":"#prerequisites","title":"Prerequisites"},{"url":"#installation","title":"Installation"},{"url":"#basic-usage","title":"Basic Usage"}]},{"url":"#contributing-","title":"Contributing ü§ù"},{"url":"#license-","title":"License üìÑ"},{"url":"#citation-","title":"Citation üìö"},{"url":"#contact-","title":"Contact üìß"}]}]},"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/56133/shake_preview.png","srcSet":"/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/56c0d/shake_preview.png 750w,\n/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/56133/shake_preview.png 759w","sizes":"(min-width: 759px) 759px, 100vw"},"sources":[{"srcSet":"/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/40255/shake_preview.webp 750w,\n/site/static/8e42b50c532c0a7cb2c68beef3e9abf4/eb21c/shake_preview.webp 759w","type":"image/webp","sizes":"(min-width: 759px) 759px, 100vw"}]},"width":1000,"height":600.7905138339921}}}},"fields":{"slug":{"html":"\n# Advanced Regularization Techniques for Neural Networks üß†\n\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## Introduction üéØ\n\nThis research explores advanced regularization techniques for neural networks, specifically focusing on improving model generalization in image classification tasks. We investigate the effectiveness of Shake-Shake regularization, Mixup, and Cutout techniques when applied to Convolutional Neural Networks (CNNs) and ResNet architectures.\n\n## Project Overview üìä\n\nOur work addresses the persistent challenge of overfitting in deep learning models through the implementation and comparison of various regularization strategies. Using the CIFAR-10 dataset as our testing ground, we demonstrate significant improvements in model performance and generalization capabilities.\n\n### Key Features üåü\n\n- Implementation of multiple regularization techniques\n- Comparative analysis across different model architectures\n- Comprehensive performance metrics and evaluation\n- Modular and extensible codebase\n- Detailed documentation and examples\n\n## Technical Implementation üíª\n\n### Basic CNN Architecture\n\nThe foundational CNN model implements the following key components:\n\n```python\nclass BasicCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Core convolution operation\n        # F_ij = sum(sum(I[i+m][j+n] * K[m][n]))\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.fc = nn.Linear(128 * 8 * 8, 10)\n        \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n```\n\n### Advanced Regularization Implementations\n\n#### Cutout\n```python\ndef cutout(image, mask_size):\n    \"\"\"Applies random cutout augmentation to the input image.\n    \n    Args:\n        image: Input tensor of shape (C, H, W)\n        mask_size: Size of the square mask\n    Returns:\n        Augmented image tensor\n    \"\"\"\n    h, w = image.size(1), image.size(2)\n    mask = torch.ones_like(image)\n    x = torch.randint(0, w - mask_size, (1,))\n    y = torch.randint(0, h - mask_size, (1,))\n    mask[:, y:y + mask_size, x:x + mask_size] = 0\n    return image * mask\n```\n\n#### Mixup\n```python\ndef mixup_data(x, y, alpha=1.0):\n    \"\"\"Performs mixup on the input data and labels.\n    \n    Args:\n        x: Input data tensor\n        y: Target labels\n        alpha: Mixup interpolation coefficient\n    Returns:\n        Mixed input, pair of targets, and lambda\n    \"\"\"\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size)\n    \n    mixed_x = lam * x + (1 - lam) * x[index]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n```\n\n#### Shake-Shake Regularization\n```python\nclass ShakeShakeBlock(nn.Module):\n    \"\"\"Implementation of Shake-Shake regularization block.\"\"\"\n    def forward(self, x):\n        if self.training:\n            alpha = torch.rand(1)\n            beta = torch.rand(1)\n        else:\n            alpha = beta = 0.5\n            \n        y = alpha * self.branch1(x) + (1 - alpha) * self.branch2(x)\n        return x + beta * y\n```\n\n## Performance Results üìà\n\nOur experiments yielded the following results:\n\n| Model | Accuracy | Precision | Recall | F1 Score |\n|-------|----------|-----------|---------|-----------|\n| ResNet50 (Base) | 92% | 91% | 90% | 90.5% |\n| + Shake-Shake | 93% | 92% | 91% | 91.5% |\n| Basic CNN | 78% | 75% | 77% | 76% |\n| + Advanced Regularization | 88% | 86% | 87% | 85.5% |\n\n### Key Findings üîç\n\n1. Shake-Shake regularization provided consistent performance improvements across all metrics\n2. Combined regularization techniques showed complementary benefits\n3. Deeper architectures demonstrated superior learning capacity\n4. Regularization significantly improved model generalization\n\n## Getting Started üöÄ\n\n### Prerequisites\n\n```bash\npython >= 3.8\ntorch >= 1.8.0\ntorchvision >= 0.9.0\nnumpy >= 1.19.2\n```\n\n### Installation\n\n```bash\ngit clone https://github.com/cawley/neural-regularization\ncd neural-regularization\npip install -r requirements.txt\n```\n\n### Basic Usage\n\n```python\nfrom models import ResNet50WithShakeShake\nfrom regularization import cutout, mixup_data\n\n# Initialize model\nmodel = ResNet50WithShakeShake()\n\n# Training with regularization\nfor inputs, targets in train_loader:\n    # Apply cutout\n    inputs = cutout(inputs, mask_size=16)\n    \n    # Apply mixup\n    inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n    \n    # Forward pass with shake-shake enabled\n    outputs = model(inputs)\n```\n\n## Contributing ü§ù\n\nWe welcome contributions! Please read our [Contributing Guidelines](CONTRIBUTING.md) before submitting pull requests.\n\n## License üìÑ\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Citation üìö\n\nIf you use this code in your research, please cite:\n\n```bibtex\n@article{cawley2023regularization,\n  title={Comparative Analysis of Shake-Shake Regularization in ResNet-Like Architecture},\n  author={Cawley, Liam},\n  journal={arXiv preprint},\n  year={2023}\n}\n```\n\n## Contact üìß\n\nLiam Cawley - cawleyl@umich.edu\n\nProject Link: [https://github.com/cawley/neural-regularization](https://github.com/cawley/neural-regularization)","htmlEncrypted":"","nonce":"","timeToRead":null,"title":"Comparative Analysis of Shake-Shake Regularization in a ResNet-Like Architecture for CIFAR-10 Image Classification","date":"1701388800000","tags":["deep-learning","regularization","resnet","neural-networks","computer-vision"],"path":"research/shake-shake-regularization","excerpt":"Investigation of effectiveness of non-ERM based methods in improving ResNet performance on image classification tasks. In the field of machine learning, especially in image classification, the challenge of overfitting and underperformance on new data is persistent. This research addresses the critical need for effective regularization techniques that enhance the generalization ability of models without compromising their performance. Utilizing the CIFAR-10 dataset, we evaluate the effectiveness of techniques such as Shake-Shake, Mixup, and Cutout in improving the performance of Convolutional Neural Networks (CNNs) and Residual Networks (ResNet).","links":[{"name":"paper","url":"/files/3bf41cf28ee3327689c4a570724dd670/shake-shake.pdf"},{"name":"code","url":"https://github.com/"}],"commit":0,"type":"research"}},"internal":{"contentFilePath":"/home/runner/work/site/site/example/content/research/shake-shake-regularization/index.md"}}},"pageContext":{"contentFilePath":"/home/runner/work/site/site/example/content/research/shake-shake-regularization/index.md","postPath":"research/shake-shake-regularization","translations":[{"hreflang":"en","path":"/research/shake-shake-regularization"}],"frontmatter":{"title":"Comparative Analysis of Shake-Shake Regularization in a ResNet-Like Architecture for CIFAR-10 Image Classification","tags":["deep-learning","regularization","resnet","neural-networks","computer-vision"],"date":"2023-12-01T00:00:00.000Z","venue":"University Research Project under Qing Qu","authors":[{"name":"Liam Cawley","url":"mailto:cawleyl@umich.edu"}],"path":"research/shake-shake-regularization","excerpt":"Investigation of effectiveness of non-ERM based methods in improving ResNet performance on image classification tasks. In the field of machine learning, especially in image classification, the challenge of overfitting and underperformance on new data is persistent. This research addresses the critical need for effective regularization techniques that enhance the generalization ability of models without compromising their performance. Utilizing the CIFAR-10 dataset, we evaluate the effectiveness of techniques such as Shake-Shake, Mixup, and Cutout in improving the performance of Convolutional Neural Networks (CNNs) and Residual Networks (ResNet).","selected":false,"cover":"./shake_preview.png","links":[{"name":"paper","file":"./shake-shake.pdf"},{"name":"code","url":"https://github.com/"}],"priority":5}}},"staticQueryHashes":["1552981879","2158328490","3013679938"],"slicesMap":{}}