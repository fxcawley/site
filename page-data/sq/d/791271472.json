{"data":{"allTag":{"edges":[{"node":{"name":"gatsby","color":"lime","path":"/tags/gatsby"}},{"node":{"name":"blog","color":"orange","path":"/tags/blog"}},{"node":{"name":"test1","color":"geekblue","path":"/tags/test1"}},{"node":{"name":"test2","color":"purple","path":"/tags/test2"}},{"node":{"name":"test3","color":"purple","path":"/tags/test3"}},{"node":{"name":"autonomous-robotics","color":"volcano","path":"/tags/autonomous-robotics"}},{"node":{"name":"flight","color":"gold","path":"/tags/flight"}},{"node":{"name":"ardupilot","color":"volcano","path":"/tags/ardupilot"}},{"node":{"name":"embedded-systems","color":"gold","path":"/tags/embedded-systems"}},{"node":{"name":"metaheuristic-optimization","color":"green","path":"/tags/metaheuristic-optimization"}},{"node":{"name":"computer-vision","color":"magenta","path":"/tags/computer-vision"}},{"node":{"name":"neural-networks","color":"red","path":"/tags/neural-networks"}},{"node":{"name":"research","color":"red","path":"/tags/research"}},{"node":{"name":"deep-learning","color":"magenta","path":"/tags/deep-learning"}},{"node":{"name":"machine-learning","color":"green","path":"/tags/machine-learning"}},{"node":{"name":"language-models","color":"lime","path":"/tags/language-models"}},{"node":{"name":"optimization","color":"cyan","path":"/tags/optimization"}},{"node":{"name":"neurips","color":"cyan","path":"/tags/neurips"}},{"node":{"name":"super-resolution","color":"geekblue","path":"/tags/super-resolution"}},{"node":{"name":"ddpm","color":"orange","path":"/tags/ddpm"}},{"node":{"name":"regularization","color":"blue","path":"/tags/regularization"}},{"node":{"name":"resnet","color":"blue","path":"/tags/resnet"}}]},"allMdx":{"edges":[{"node":{"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/d47952e4bd91c70708919d79f77a2346/3ea76/lora_preview.png","srcSet":"/static/d47952e4bd91c70708919d79f77a2346/3ea76/lora_preview.png 320w,\n/static/d47952e4bd91c70708919d79f77a2346/a2ff0/lora_preview.png 712w","sizes":"(min-width: 320px) 320px, 100vw"},"sources":[{"srcSet":"/static/d47952e4bd91c70708919d79f77a2346/4a8d5/lora_preview.webp 320w,\n/static/d47952e4bd91c70708919d79f77a2346/76729/lora_preview.webp 712w","type":"image/webp","sizes":"(min-width: 320px) 320px, 100vw"}]},"width":320,"height":180}}}},"fields":{"slug":{"date":"1733011200000","venue":"ICLR Workshop 2025","authors":["[Liam Cawley](https://cawleyl.github.io)"],"path":"research/lorambo","title":"LoRAMBo: Fighting LoRA Memory Bottlenecks with Optimized Rank Selection","tags":["deep-learning","machine-learning","language-models","optimization","neurips"],"excerpt":"Low-Rank Adaptation (LoRA) efficiently fine-tunes large pre-trained language models through low-rank weight updates, significantly reducing memory usage compared to full fine-tuning. However, the problem of how to optimally allocate low-rank parameters across model layers remains challenging. This paper presents an extended theoretical framework that unifies classical matrix approximation perspectives with data- and curvature-aware approaches, developing both offline and online rank-allocation algorithms with near-optimality guarantees. Our results significantly advance the theoretical understanding of efficient model adaptation while providing strong empirical evidence for adopting curvature- and data-aware rank selection strategies in large-scale applications.","selected":true,"priority":0,"links":[{"name":"paper","url":"/files/6656fcf836373e7122f4a7f83cdf7beb/lora_master.pdf"},{"name":"code","url":"https://github.com/cawley/lorambo"}]}},"internal":{"contentFilePath":"/home/runner/work/site/site/example/content/research/lorambo/index.md"}}}]}}}